{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6093b1-1b60-4fb7-9210-79f943a8cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing for Machine Learning - Python Code Reference\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c946d7b-f8c5-4b3d-87ae-c6aac008aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV file\n",
    "data = pd.read_csv(\"student.csv\")\n",
    "\n",
    "# Get dataset dimensions (rows, columns)\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "\n",
    "# View first 5 rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "\n",
    "# Get statistical summary of numerical columns\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b0937-27bb-40ab-a3c5-fdc4cbd48c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Missing Values\n",
    "print(\"\\nNon-null values in each column:\")\n",
    "print(data.count())\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Basic Statistics for Missing Value Handling\n",
    "print(\"\\nMean of marks column:\", data['marks'].mean())\n",
    "print(\"Maximum age:\", data['age'].max())\n",
    "print(\"Mode of class column:\", data['class'].mode())\n",
    "\n",
    "# Missing Value Treatment Methods\n",
    "\n",
    "# Method 1: Remove rows with any missing values\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "# Method 2: Fill all missing values with a constant (e.g., 0)\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Method 3: Forward fill (use previous valid observation)\n",
    "data_filled = data.fillna(method='pad')\n",
    "\n",
    "# Method 4: Backward fill (use next valid observation)\n",
    "data_filled = data.fillna(method='backfill')\n",
    "\n",
    "# Method 5: Fill numerical column with median\n",
    "data['marks'] = data['marks'].fillna(data['marks'].median())\n",
    "\n",
    "# Method 6: Fill numerical column with mean\n",
    "data['marks'] = data['marks'].fillna(data['marks'].mean())\n",
    "\n",
    "# Method 7: Fill categorical column with mode\n",
    "data['class'] = data['class'].fillna(data['class'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6663a4-7d73-4860-a938-49ce7d82e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are defined (features and target)\n",
    "# X = data.drop('target_column', axis=1)\n",
    "# y = data['target_column']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# 75% train, 25% test with fixed random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.25, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9591e-f42f-4adf-9739-fac8ab2067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaling (Standardization)\n",
    "# Creates features with mean=0 and std=1\n",
    "scaler_standard = StandardScaler()\n",
    "X_standard_scaled = scaler_standard.fit_transform(X)\n",
    "\n",
    "# Min-Max Scaling (Normalization)\n",
    "# Scales features to a range between 0 and 1\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax_scaled = scaler_minmax.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8bde9-234f-4ba6-82bd-880c2376babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Continuous Features to Binary\n",
    "# Values above threshold become 1, below become 0\n",
    "binarizer = Binarizer(threshold=30)\n",
    "X_binned = binarizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610e27a-f07a-40dc-8abd-d689bbe77213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "# Converts categorical values to numerical labels\n",
    "le = LabelEncoder()\n",
    "data['Name'] = le.fit_transform(data['Name'])\n",
    "\n",
    "# One-Hot Encoding\n",
    "# Creates binary columns for each category\n",
    "data_encoded = pd.get_dummies(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e5120-5a84-453c-bda7-bd5a43e38333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filepath, target_column):\n",
    "    # 1. Load data\n",
    "    data = pd.read_csv(filepath)\n",
    "    print(f\"Dataset loaded with shape: {data.shape}\")\n",
    "    \n",
    "    # 2. Explore data\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(data.head())\n",
    "    print(\"\\nDataset info:\")\n",
    "    print(data.describe())\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(data.isnull().sum())\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    # For numerical columns: fill with mean\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numerical_cols:\n",
    "        if col != target_column:\n",
    "            data[col] = data[col].fillna(data[col].mean())\n",
    "    \n",
    "    # For categorical columns: fill with mode\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col != target_column:\n",
    "            data[col] = data[col].fillna(data[col].mode()[0])\n",
    "    \n",
    "    # 4. Encode categorical variables\n",
    "    le_dict = {}\n",
    "    for col in categorical_cols:\n",
    "        if col != target_column:\n",
    "            le = LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col])\n",
    "            le_dict[col] = le\n",
    "    \n",
    "    # 5. Split features and target\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # 6. Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 7. Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, le_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165015b-ac10-4c4b-857a-e495b764e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the preprocessing pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual file path and target column name\n",
    "    # X_train, X_test, y_train, y_test, scaler, encoders = preprocess_data(\"your_dataset.csv\", \"target_column\")\n",
    "    \n",
    "    # For individual operations, you can use the functions above\n",
    "    # Make sure to always fit on training data and transform both train and test\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
